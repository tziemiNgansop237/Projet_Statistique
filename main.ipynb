{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: 'Georgia', serif; text-align: center; font-size: 30px; color:rgb(52, 152, 219); text-shadow: 2px 2px 5px rgba(0, 0, 0, 0.3)\">Préparation des données</h1>\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastparquet\n",
    "!pip install rpy2\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chemin de la Base de données; \n",
    "#commentez l'url et vous remplacez par votre chemin sans supprimer!!!!!\n",
    "\n",
    "url=\"/home/onyxia/work/Projet_Statistique/data/base_Edu.parquet\"\n",
    "base_Edu=pd.read_parquet(url)\n",
    "base_Edu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_Edu.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données compte donc 477 variables dont 72 de type Int, 254 de type boolean, 1 de type datetime et 150 de type float soit au total 254 variables qualitatives (**boolean**) et 221 variables quantitatives(**float + int** en excluant les identifiants) sans oublier la variable de type **datetime**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commencons par épurer la base données avant de passer à la réduction de dimension. On s'intéressera aux valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value=(base_Edu.isnull().sum()/len(base_Edu))\n",
    "missing_value[missing_value>0]\n",
    "missing_value[missing_value>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données comporte donc 404 variables avec valeurs manquantes sur le total des 476 variables de la base ce qui n'est pas du tout négligeable. Voyons combien de valeurs manquantes at-on par type de variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(base_Edu.select_dtypes(\"int\").isnull().any().sum(),\" variables de type int ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"float\").isnull().any().sum(),\" variables de type float ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"boolean\").isnull().any().sum(),\" variables de type boolean ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"datetime\").isnull().any().sum(),\" variables de type datetime ont des valeurs manquantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seule variable de type datetime n'a donc pas de valeurs manquantes.  Analysons de plus près les proportions de valeurs manquantes des autres types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_bool=base_Edu.select_dtypes(\"boolean\").isnull().sum()/len(base_Edu)\n",
    "miss_bool[miss_bool>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables de type boolean comporte pratiquement tous assez de valeurs manquantes avec un minimum de 69% et un maximum de 100% du total des observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs manquantes de la base ne sont pas complètement aléatoires. Elles ne sont pas pour la plupart le fruit d'une mauvaise collecte de données. La présence de valeurs manquantes dans cette base est due à diverses raisons notamment le fait que plusieurs individus de la base n'ont pas encore passé d'examen pour le permis. Plusieurs questions(variables) n'ont de sens que dans le cas où le premier examen est passé. Il y a donc une part d'information apporté par ces valeurs manquantes que nous devons inclure dans nos analyses. Pour les variables booléenne, nous pouvons régler ce prblème c'est à dire prendre en compte l'information apportée par ces valeurs manquantes en transformant les variables booléennes en variable catégorielles en considérant les valeurs manquantes comme une catégorie(True=1,False=0 et NA=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(base_Edu.select_dtypes('category'))\n",
    "def encode_column_with_na(col):\n",
    "    mapping = {True: 1, False: 0, pd.NA: 2}  # Encoder les valeurs booléennes et <NA>\n",
    "    return col.map(mapping)\n",
    "colboo=base_Edu.select_dtypes(\"boolean\").columns\n",
    "for col in colboo:\n",
    "    base_Edu[col] = encode_column_with_na(base_Edu[col]).astype(\"category\")\n",
    "#base_Edu[\"is_first_exam_success\"].cat.categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant aux variables quantitatives (int+float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_quant=base_Edu.select_dtypes(include=[\"int\",\"float\"]).isnull().sum()/len(base_Edu)\n",
    "miss_quant[miss_quant>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les variables quantitatives, la distribution des valeurs manquantes est plus étendue. On enregistre 151 variables quantitatives avec valeurs manquantes. On a un minimum de 4% et un maximum de 94% de valeurs manquantes par rapport au total d'observation(nous rappelons 200000 obervations). La nature des variables et la distribution des valeurs manquantes dans ce cas ne facilite pas la capture de l'information apportée par la présence de ces valeurs manquantes. On pourrait opter pour une imputation par KNN ou par imputation itérative (MICE) mais cela risque de modifier et significativement les corrélations existant déjà entre les variables introduisant ainsi un important biais et au vue de la taille de l'echantillon, cela demanderait un temps d'excution énorme. On choisit ici donc de procéder à une imputation par la médiane après suppression des variables avant un taux élevé de valeurs manquantes pour minimiser le biais. Cela permettra de ne pas trop distordre les distributions existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_quant[miss_quant<=0.20].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choisissons le seuil de 20% de valeurs manquantes. Ce seuil nous parait raisonnable puisqu'il est relativement faible et n'entraine pas la suppression d'un grand nombre de variables quantitatives (118 sont conservées sur un total de 151)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des variables avec plus de 20% de valeurs manquantes\n",
    "base_Edu=base_Edu.drop(columns=miss_quant[miss_quant>0.20].index)\n",
    "#Imputation par la médiane\n",
    "col=base_Edu.select_dtypes(exclude='category').columns\n",
    "base_Edu[col] = base_Edu[col].fillna(base_Edu[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_Edu.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = base_Edu.select_dtypes(exclude=['datetime64'])  # Colonnes non datetime64\n",
    "df2 = base_Edu.select_dtypes(include=['datetime64']) #colonnes datetime\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sauvegarde des données sans var date\n",
    "df1.to_parquet('data_Edu.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus aucune valeur manquante. La base de données enfin prête, on peut passer aux opérations de réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;color:rgb(47, 99, 220);font-size: 30px\">\n",
    "<u>\n",
    "Réduction de dimension</u>\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donnée la dimension de la base de données( beaucoup de variables), ce sera très complexe voir impossible de faire des analyses ou entrainer un modèle directement sur cette base. Il est donc important voir optimale de faire au préalable une réduction de dimension. Pour ce faire, nous allons utiliser différentes approches de réduction de dimension, faire des comparaisons puis retenir la meilleure approche dans ce cas. Il s'agira principalement de faire une **analyse enn composante principale (ACP)** , une **FAMD** et utiliser **un autoencodeur**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 25px\">\n",
    "<u>\n",
    "1- Analyse en composantes principales (ACP) </u>\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour effectuer l'ACP il faut restreindre la base aux variables quantitatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=base_Edu.select_dtypes(include=[\"int\",\"float\"])\n",
    "data=data.drop([\"unique_learner_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, nous allons standardiser les variables afin de faire qu'elles aient toutes la même échelle car l'ACP dépend fortement de l'échelle des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =StandardScaler()\n",
    "X_scaled =scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 23px\">\n",
    " a)- Choix du nombre de composantes principales\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Règle du coude**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit ici de construire la courbe de décroissance des valeurs propres et de détecter des \"coudes\" (ou \"cassures\") signalant un changement de structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "pca=PCA(n_components=15)\n",
    "pca.fit(X_scaled)\n",
    "sns.barplot(x= np.arange(1,16) , y=pca.explained_variance_ratio_*100, hue=np.arange(1,16), legend=False)\n",
    "plt.ylabel(\"Explain variance\")\n",
    "plt.xlabel(\"Principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe de ce graphique qu'il y a deux coudes possibles, l'un à la deuxième composante et l'autre à la sixième composante. La première composante principale explique 11,66% de la variance totale, la deuxième en explique 8,05%, la troisième 7,27%, et à partir de la sixième composante principale, le pourcentage de variable expliquée devient inférieur à 3%. Les facteurs restants ont donc un apport d'information négligeable.\n",
    "\n",
    "A ce graphique, on ajoute le graphique qui décrit l'évolution de la variance expliquée par les axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variances=[]\n",
    "for n in range(1,16):\n",
    "    pca =PCA(n_components=n)\n",
    "    pca.fit(X_scaled)\n",
    "    explained_variances.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1,16), explained_variances, marker='o')\n",
    "plt.xlabel(\"number of components\")\n",
    "plt.ylabel(\"cumulative explain variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce graphique montre que les 6 premières composantes principales expliquent environ 39% de l'information. Considerer deux composantes principales n'est pas une bonne idée car on perdrais le gain d'information substanciel qu'apporte les 4 composantes principales suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Test des « bâtons brisés »**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce test est du à Frontier (1976) et Legendre-Legendre (1983). Il repose sur l’idée que si l’inertie totale était dispatchée aléatoirement sur les axes, la distribution des valeurs propres suivrait la loi des \n",
    "« bâtons brisés » (broken-stick). La valeur critique pour le choix des composantes principales s'écrit comme suit:\n",
    "$$ b_{k} = \\sum_{i=k}^{p} \\frac 1 {i}$$\n",
    "\n",
    "où p représente le nombre de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "baton_coef=[]\n",
    "for i in np.arange(1,16):\n",
    "    n=0\n",
    "    for j in range(i,117):\n",
    "        n=n+(1/j)\n",
    "    baton_coef.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.DataFrame(data=pca.explained_variance_, columns=[\"eigen_values\"])\n",
    "data1[\"b_k\"]=baton_coef\n",
    "data1.index=[\"PC{}\".format(i) for i in np.arange(1,16)]\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec ce test on retient bien 6 composantes principales, car jusqu'a la sixième composante principale, la valeur propre de chaque composante est bien supérieur au seuil calculé.\n",
    "\n",
    "Nous retiendrons donc 6 composantes principales dans la suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 23px\">\n",
    "b)- Entrainement du modèle\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with 6 principal components\n",
    "pca = PCA(n_components=6)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(\"Explain variance for each component :\",pca.explained_variance_ratio_)\n",
    "print(\"cumulated variance :\",pca.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficients of the principal components \n",
    "print(\"Composantes principales (coefficients) :\")\n",
    "table_coef=pd.DataFrame(pca.components_, columns=data.select_dtypes(exclude='category').columns)\n",
    "table_coef.index=[\"PC{}\".format(i+1) for i in np.arange(0,6)]\n",
    "table_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contributions of the principal components\n",
    "loadings=pca.components_.T\n",
    "eigenvalues=pca.explained_variance_\n",
    "contributions=(loadings**2)*eigenvalues\n",
    "contrib_percent=contributions/(eigenvalues.sum())*100\n",
    "contrib_df=pd.DataFrame(contrib_percent, columns=[\"PC{}\".format(i+1) for i in np.arange(0,6)])\n",
    "contrib_df.index=data.columns\n",
    "contrib_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 23px\">\n",
    "c)- Interprétaion des axes principaux\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seront considérées comme variables les plus contributrices à la formation d'un axe principale, les variables dont la contribution est supérieur à la contribution moyenne, c'est-à-dire: $$ 100/117 \\approx 0.855 $$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib1=pd.DataFrame(data=contrib_df[\"PC1\"].sort_values(ascending=False)[:16].values, columns=[\"contribution\"]) \n",
    "contrib1.index=contrib_df[\"PC1\"].sort_values(ascending=False)[:16].index\n",
    "contrib1[\"coefficient\"]=table_coef[contrib1.index].iloc[0]\n",
    "contrib1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables qui contribuent le plus à la construction de la première composante principale sont des variables qui renseigne sur le nombre d'activité effectuer par thématique pour le permis de conduire (*notions_diverses_number_of_activities*, *mecanique_equipements_number_of_activities*,*securite_number_of_activities*, *route_number_of_activities*, *prendre_quitter_vehicule_number_of_activities*, *autres_usagers_number_of_activities*,etc.) et les statistiques qui résultent des scores en pourcentage obtenus aux différents examens notamment les quantiles d'ordre 1, 2, 3 et 4 (*score_pct__quantile__q_0_1*, *score_pct__quantile__q_0_2*, *score_pct__quantile__q_0_3*, *score_pct__quantile__q_0_4*), et .... Cet axe résume le signal et/ou la corrélation qui existe entre le nombre d'activités effectués par thématique et la distribution du score au différents examens. On pourrait donc dire qu'il traduit l'**efficacité** des étudiants.\n",
    "\n",
    "De plus, en analysant les coefficients de ces variables, ont constate qu'elles sont toutes positifs. Ainsi, un individu ayant un coefficient élevé sur cet axe, fait beaucoup d'activité sur les différentes thématiques d'apprentissage et a des scores élevés aux différents examens, ce qui signifie qu'il comprend bien ce qu'il fait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib2=pd.DataFrame(data=contrib_df[\"PC2\"].sort_values(ascending=False)[:9].values, columns=[\"contribution\"]) \n",
    "contrib2.index=contrib_df[\"PC2\"].sort_values(ascending=False)[:9].index\n",
    "contrib2[\"coefficient\"]=table_coef[contrib2.index].iloc[1]\n",
    "contrib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables qui contribuent fortement à la deuxième composante principale sont d'une part les variables qui renseignent sur le nombre de week-end où les objectifs d'étude hebdomadaire ont atteint un pourcentage donnée (90%,85%,95%,99%,75%,etc) et d'autre part les variables qui donnent le pourcentage d'objectifs d'études atteint le week-end (*nb_weeks_weekly_study_objective_reached*) et en dehors (*pct_study_objective_reached*). On pourrait dire que cet axe capture principalement l'information sur **l'intensité du travail personnel des étudiants**. \n",
    "\n",
    "De plus, le coefficient de toutes les variables qui contribuent le plus à la formation de l'axe 2 sont positifs. Donc, les individus qui ont un coefficient élevé et positif sur cet axe (se trouvant à une extrémité de l'axe) sont beaucoup investit personnellement dans la formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib3=pd.DataFrame(data=contrib_df[\"PC3\"].sort_values(ascending=False)[:8].values, columns=[\"contribution\"]) #pas terminé\n",
    "contrib3.index=contrib_df[\"PC3\"].sort_values(ascending=False)[:8].index\n",
    "contrib3[\"coefficient\"]=table_coef[contrib3.index].iloc[2]\n",
    "contrib3 #pas terminé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables qui contribuent fortement à la 3e composante principale sont les variables qui caractérisent la distribution du score en pourcentage (notamment les quantiles et ...) aux différents examens renseignées pour chaque étudiant. On peut dire que cet axe capture l'information sur **la progression des étudiants en terme de résultats**. \n",
    "\n",
    "De plus, le fait que toutes ces variables ont un coefficient positif sur l'axe signifient qu'elles varient dans le même sens que l'axe. Donc un étudiant ayant un coefficient très faible (négatif) sur cet axe, obtient de mauvais résultats aux différents examens, ce qui traduit le fait qu'ils n'assimilent pas bien les connaissances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib4=pd.DataFrame(data=contrib_df[\"PC4\"].sort_values(ascending=False)[:6].values, columns=[\"contribution\"]) #pas terminé\n",
    "contrib4.index=contrib_df[\"PC4\"].sort_values(ascending=False)[:6].index\n",
    "contrib4[\"coefficient\"]=table_coef[contrib4.index].iloc[3]\n",
    "contrib4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mesure de la dispersion du score (*score_pct_standard_deviation*)  et la variation relative du score en pourcentage autour de la moyenne (*score_pct__variation_coefficient*)  contribuent majoritairement à la formation de la 4e composante principale. Aussi, le nombre d'examens échoués (*nb_failed_series*), le total d'activités théorique (*theory_activities_total*), le minimum du score en pourcentage (*score_pct_minimum*) , le nombre de session effectué (*session_count*), contribuent également fortement à la formation de cet axe. Cette composante semble donc être fortement influencé par des variables liées à **la performance académique, à la régularité aux examens**. \n",
    "\n",
    "De l'analyse des coefficients de ces variables, on observe que les variables qui donne le minimum du score en pourcentage et la dernière localisation du minimum du score ont des coefficients négatifs. On pourrait donc dire que les individus ayant un score élevé sur cette composante sont succeptibles d'avoir des performances très irrégulières (caractérisées par une dispersion élevée des scores), rencontrent régulièrement des échecs et mettent du temps à assimiler les compétences (nombre de series échoué élevé), ce qui impliquent qu'ils doivent faire plus d'activités, d'examen et de session (**theory_activities_total**,**session_count**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrib5=pd.DataFrame(data=contrib_df[\"PC5\"].sort_values(ascending=False)[:4].values, columns=[\"contribution\"]) #pas terminé\n",
    "contrib5.index=contrib_df[\"PC5\"].sort_values(ascending=False)[:4].index\n",
    "contrib5[\"coefficient\"]=table_coef[contrib5.index].iloc[4]\n",
    "contrib5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le maximum (*minutes_between_sessions_max*), la moyenne (*minutes_between_sessions_avg*), l'écart type (*minutes_between_sessions_std*) qui mesurent **la régularité et la concentration des sessions d'apprentissage** contribuent fortement à la formation de la cinquième composante principale. Egalement, le nombre de jour entre la première activité et la dernière activité (*days_between_first_and_last_activities*) qui mesure **l'engagement totale de l'étudiant dans la formation**, contribue fortement à la formation de cet axe.\n",
    "\n",
    "Les coefficients de ces variables sont tous positifs, ce qui signifie que la composante varie dans le même sens que ces variables. Donc, les individus ayant un coefficient positif élevé sur cet axe (se situant à une extrémité de l'axe) ont une formation étalée sur une longue période, des sessions d'apprentissage très irrégulières, avec de longues périodes d'inactivité entre chaque session (un écart type élevé suggère des sessions très espacées dans le temps, et une médiane élevée indique des sessions moins fréquentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation à revoir\n",
    "contrib6=pd.DataFrame(data=contrib_df[\"PC6\"].sort_values(ascending=False)[:4].values, columns=[\"contribution\"]) #pas terminé\n",
    "contrib6.index=contrib_df[\"PC6\"].sort_values(ascending=False)[:4].index\n",
    "contrib6[\"coefficient\"]=table_coef[contrib6.index].iloc[5]\n",
    "contrib6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur la base du critère que l'on s'est fixé, on constate qu'aucune variable ne contribuent fortement à la formation de cet axe. Néamoins, si l'on considère les 4 premières variables, c'est-à-dire le nombre d'activité par session en moyenne (*n_activity_per_session_avg*), le pourcentage de series par thème (*percentage_serie_theme*), le pourcentage de week-end où l'étudiant n'avais aucun objectif d'étude (*pct_no_objective_weeks*), le pourcentage d'objectifs d'étude non atteint (*pct_study_objective_not_reached*), on peut dire que cet axe mesure **l'engagement des étudiants**.\n",
    "\n",
    "En analysant le coefficient de ces variables, ont constate que la variable *pct_no_objective_weeks* a un coefficient négatif. Donc les individus ayant des coefficients élevés positives sur cet axe font en moyenne un nombre élevé d'activité par session, se fixent des objectifs d'étude le week-end régulièrement mais atteignent rarement leur objectifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 23px\">\n",
    "d)- Graphique des individus\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons représenter les 20 premiers individus de la base sur les différents axes factorielles afin de voir concrètement ce qu'ils traduisent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca=X_pca[1:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "#plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "labels = data.iloc[1:20,:].index # Utiliser les indices du DataFrame comme étiquettes\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X_pca[i, 0], X_pca[i, 1]))\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.title(\"Graph of individuals\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 2], X_pca[:, 3])\n",
    "labels = data.iloc[1:20,:].index # Utiliser les indices du DataFrame comme étiquettes \n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X_pca[i, 2], X_pca[i, 3]))\n",
    "plt.xlabel(\"third principal component\")\n",
    "plt.ylabel(\"fourth principal component\")\n",
    "plt.title(\"Graph of individuals\")\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.scatter(X_pca[:, 4], X_pca[:, 5])\n",
    "labels = data.iloc[1:20,:].index # Utiliser les indices du DataFrame comme étiquettes \n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (X_pca[i, 4], X_pca[i, 5]))\n",
    "plt.xlabel(\"Fifth principal component\")\n",
    "plt.ylabel(\"Sixth principal component\")\n",
    "plt.title(\"Graph of individuals\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considérons l'individu numéro 17. Il a un coefficient élevé sur les deux premiers axes factoriels, ce qui signifie qu'il fait un grand nombre d'activités par thématique d'apprentissage et à de bons scores aux différentes session d'évaluation. De plus il a un coefficient négatif relativement faible sur le reste des axes factoriels, ce qui signifie qu'il a des scores peu dispersé autour de la moyenne, il a de bonne performance académique et est plus ou moins régulier aux sessions d'apprentissage. En bref, c'est un bon élève.\n",
    "\n",
    "Par contre, l'individu numéro 15 a un coefficient négatif sur le premier axe factoriel, proche de 0 sur le deuxième axe factoriel et négatif sur le troisième axe factoriel ce qui signifie qu'il ne fait relativement peu d'activités par thématique d'apprentissage et n'a pas de bonnes performances académique. De plus, il a un coefficient négatif sur le cinquième axe factoriel et proche de 0 sur le quatrième et sixième axe factoriel, ce qui traduit le fait qu'il est moin régulier aux examens et ses sessions d'apprentissage sont moins concentrées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 25px\">\n",
    "<u>\n",
    "2- Analyse Factorielle des Composantes Mixtes (FAMD) </u>\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df= pd.read_parquet(\"data_Edu.parquet\")\n",
    "df_sample=df.sample(n=1000, random_state=42)\n",
    "df_sample.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io\n",
    "\n",
    "#sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "#definir l'emplacement de l'environnement r et des librairies utilisés\n",
    "os.environ['R_HOME'] = r\"C:\\Program Files\\R\\R-4.4.2\"  # Exemple : '/home/user/anaconda3/envs/mon_env/lib/R'\n",
    "os.environ['R_LIBS_USER'] = r\"C:\\Users\\damso\\AppData\\Local\\R\\win-library\\4.4\"\n",
    "# Optionnel : Ajouter le chemin des DLLs de R pour éviter des erreurs de chargement\n",
    "#os.environ['PATH'] += os.pathsep + r\"C:\\Program Files\\R\\R-4.4.2\\bin\\x64\"\n",
    "\n",
    "# Importer rpy2 et activer la conversion pandas2ri\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "#ro.r('library(factoextra)')\n",
    "#ro.r('library(FactoMineR)')\n",
    "\n",
    "# Charger les données dans R\n",
    "\n",
    "base_AE = df_sample.reset_index(drop=True)\n",
    "\n",
    "ro.globalenv['data'] = pandas2ri.py2rpy(base_AE)\n",
    "\n",
    "# Récupérer les colonnes de type 'category'\n",
    "category_columns = base_AE.select_dtypes(include=['category']).columns\n",
    "\n",
    "# Convertir les noms des colonnes en liste pour les passer à R\n",
    "ro.globalenv['category_columns'] = category_columns.tolist()\n",
    "\n",
    "\n",
    "ro.r('''\n",
    "    # Vérifier si les bibliothèques sont installées et les charger\n",
    "    #if (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n",
    "    #if (!require(\"FactoMineR\")) install.packages(\"FactoMineR\")\n",
    "    #if (!require(\"factoextra\")) install.packages(\"factoextra\")\n",
    "     \n",
    "    # Récupérer les noms des colonnes de type 'category' de Python\n",
    "    category_columns <- as.character(category_columns)\n",
    "\n",
    "    # Appliquer la conversion en factor sur ces colonnes\n",
    "    data[category_columns] <- lapply(data[category_columns], as.factor)\n",
    "    # Afficher les niveaux de chaque variable factor\n",
    "    # Filtrer uniquement les variables de type factor\n",
    "    #library(RColorBrewer)\n",
    "    #library(ggplot2)\n",
    "    library(FactoMineR)\n",
    "    library(factoextra)\n",
    "    # Exemple pour toutes les variables qualitatives\n",
    "    result <- FAMD(data, ncp = 10, graph = FALSE)\n",
    "\n",
    "    res <-summary(result)\n",
    "    result_df <- as.data.frame(result$ind$coord) # Récupérer les coordonnées des individus\n",
    "    result_var_df <- as.data.frame(result$var$coord) # Récupérer les coordonnées des individus\n",
    "''')\n",
    "# #Récupérer les résultats en Python\n",
    "result_df = ro.r('result_df')\n",
    "result_var_df = ro.r('result_var_df')\n",
    "res = ro.r('res')\n",
    "print(res)\n",
    "\n",
    "result_df = pandas2ri.rpy2py(result_df)\n",
    "result_var_df = pandas2ri.rpy2py(result_var_df)\n",
    "result_var_df.info()\n",
    "#print(\"Résultats de la FAMD :\\n\", result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 10 variables ont été conservés et contiennent environ % de variance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p style=\"font-family: 'Georgia', serif; text-align: center;font-size: 25px\">\n",
    "<u>\n",
    "3- Auto Encoder </u>\n",
    "</p>\n",
    "<br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
