{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_learner_id</th>\n",
       "      <th>days_between_signup_and_first_activity</th>\n",
       "      <th>days_between_order_and_first_activity</th>\n",
       "      <th>first_theory_activity_date</th>\n",
       "      <th>days_between_first_and_last_activities</th>\n",
       "      <th>chapter_before_success_count</th>\n",
       "      <th>serie_before_success_count</th>\n",
       "      <th>quiz_before_success_count</th>\n",
       "      <th>theory_activities_total</th>\n",
       "      <th>weekly_study_objective</th>\n",
       "      <th>...</th>\n",
       "      <th>nb_weeks_weekly_study_objective_reached_80pct</th>\n",
       "      <th>nb_weeks_weekly_study_objective_reached_75pct</th>\n",
       "      <th>nb_weeks_weekly_study_objective_reached_70pct</th>\n",
       "      <th>nb_weeks_weekly_study_objective_not_reached</th>\n",
       "      <th>nb_weeks_no_activity</th>\n",
       "      <th>nb_weeks_no_weekly_study_objective</th>\n",
       "      <th>pct_study_objective_reached</th>\n",
       "      <th>pct_study_objective_not_reached</th>\n",
       "      <th>pct_study_objective_no_activity</th>\n",
       "      <th>pct_no_objective_weeks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3838161406066513919</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-17 15:00:23.000000</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5829430568065349352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-07 12:02:53.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7677180318050051984</td>\n",
       "      <td>787</td>\n",
       "      <td>788</td>\n",
       "      <td>2022-05-10 01:35:49.000000</td>\n",
       "      <td>295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5150597998144597550</td>\n",
       "      <td>615</td>\n",
       "      <td>616</td>\n",
       "      <td>2024-04-18 04:56:03.262330</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>969323468959211246</td>\n",
       "      <td>169</td>\n",
       "      <td>170</td>\n",
       "      <td>2019-03-19 12:32:56.000000</td>\n",
       "      <td>554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>1306435320134834461</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>2021-02-25 10:11:13.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>5732789662758255879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-06 15:46:14.000000</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>3325713948509922176</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>2019-07-21 18:24:08.000000</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>7446387329160512320</td>\n",
       "      <td>68</td>\n",
       "      <td>57</td>\n",
       "      <td>2021-10-28 19:09:58.000000</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>120</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>-426531306216685800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-07-25 16:20:25.000000</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>240</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 477 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           unique_learner_id  days_between_signup_and_first_activity  \\\n",
       "0        3838161406066513919                                      41   \n",
       "1        5829430568065349352                                       0   \n",
       "2        7677180318050051984                                     787   \n",
       "3       -5150597998144597550                                     615   \n",
       "4         969323468959211246                                     169   \n",
       "...                      ...                                     ...   \n",
       "199995   1306435320134834461                                       0   \n",
       "199996   5732789662758255879                                       0   \n",
       "199997   3325713948509922176                                      17   \n",
       "199998   7446387329160512320                                      68   \n",
       "199999   -426531306216685800                                       0   \n",
       "\n",
       "        days_between_order_and_first_activity first_theory_activity_date  \\\n",
       "0                                           0 2017-12-17 15:00:23.000000   \n",
       "1                                           0 2022-07-07 12:02:53.000000   \n",
       "2                                         788 2022-05-10 01:35:49.000000   \n",
       "3                                         616 2024-04-18 04:56:03.262330   \n",
       "4                                         170 2019-03-19 12:32:56.000000   \n",
       "...                                       ...                        ...   \n",
       "199995                                     -4 2021-02-25 10:11:13.000000   \n",
       "199996                                      0 2017-12-06 15:46:14.000000   \n",
       "199997                                      9 2019-07-21 18:24:08.000000   \n",
       "199998                                     57 2021-10-28 19:09:58.000000   \n",
       "199999                                      0 2021-07-25 16:20:25.000000   \n",
       "\n",
       "        days_between_first_and_last_activities  chapter_before_success_count  \\\n",
       "0                                          265                             0   \n",
       "1                                           27                             0   \n",
       "2                                          295                             0   \n",
       "3                                          138                             0   \n",
       "4                                          554                             0   \n",
       "...                                        ...                           ...   \n",
       "199995                                       7                             0   \n",
       "199996                                      45                             0   \n",
       "199997                                     229                             0   \n",
       "199998                                     172                             0   \n",
       "199999                                      14                             0   \n",
       "\n",
       "        serie_before_success_count  quiz_before_success_count  \\\n",
       "0                                3                          0   \n",
       "1                                0                          0   \n",
       "2                                0                          0   \n",
       "3                                0                          0   \n",
       "4                                0                          0   \n",
       "...                            ...                        ...   \n",
       "199995                           0                          0   \n",
       "199996                           0                          0   \n",
       "199997                           0                          0   \n",
       "199998                         242                          0   \n",
       "199999                           0                          0   \n",
       "\n",
       "        theory_activities_total  weekly_study_objective  ...  \\\n",
       "0                             3                    <NA>  ...   \n",
       "1                            11                    <NA>  ...   \n",
       "2                            73                     240  ...   \n",
       "3                            75                    <NA>  ...   \n",
       "4                            41                     120  ...   \n",
       "...                         ...                     ...  ...   \n",
       "199995                       11                     240  ...   \n",
       "199996                        7                    <NA>  ...   \n",
       "199997                       83                    <NA>  ...   \n",
       "199998                      310                     120  ...   \n",
       "199999                      145                     240  ...   \n",
       "\n",
       "        nb_weeks_weekly_study_objective_reached_80pct  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "199995                                              0   \n",
       "199996                                              0   \n",
       "199997                                              0   \n",
       "199998                                              0   \n",
       "199999                                              0   \n",
       "\n",
       "        nb_weeks_weekly_study_objective_reached_75pct  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "199995                                              0   \n",
       "199996                                              0   \n",
       "199997                                              0   \n",
       "199998                                              0   \n",
       "199999                                              0   \n",
       "\n",
       "        nb_weeks_weekly_study_objective_reached_70pct  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   0   \n",
       "4                                                   0   \n",
       "...                                               ...   \n",
       "199995                                              0   \n",
       "199996                                              0   \n",
       "199997                                              0   \n",
       "199998                                              0   \n",
       "199999                                              0   \n",
       "\n",
       "        nb_weeks_weekly_study_objective_not_reached  nb_weeks_no_activity  \\\n",
       "0                                                 0                     0   \n",
       "1                                                 0                     0   \n",
       "2                                                 1                     0   \n",
       "3                                                 0                     0   \n",
       "4                                                 1                     0   \n",
       "...                                             ...                   ...   \n",
       "199995                                            1                     0   \n",
       "199996                                            0                     0   \n",
       "199997                                            0                     0   \n",
       "199998                                           15                     0   \n",
       "199999                                            1                     0   \n",
       "\n",
       "        nb_weeks_no_weekly_study_objective  pct_study_objective_reached  \\\n",
       "0                                        1                          0.0   \n",
       "1                                        1                          0.0   \n",
       "2                                        0                          0.0   \n",
       "3                                        1                          0.0   \n",
       "4                                        0                          0.0   \n",
       "...                                    ...                          ...   \n",
       "199995                                   0                          0.0   \n",
       "199996                                   1                          0.0   \n",
       "199997                                   1                          0.0   \n",
       "199998                                   0                          0.0   \n",
       "199999                                   0                          0.0   \n",
       "\n",
       "        pct_study_objective_not_reached  pct_study_objective_no_activity  \\\n",
       "0                                   0.0                              0.0   \n",
       "1                                   0.0                              0.0   \n",
       "2                                 100.0                              0.0   \n",
       "3                                   0.0                              0.0   \n",
       "4                                 100.0                              0.0   \n",
       "...                                 ...                              ...   \n",
       "199995                            100.0                              0.0   \n",
       "199996                              0.0                              0.0   \n",
       "199997                              0.0                              0.0   \n",
       "199998                            100.0                              0.0   \n",
       "199999                            100.0                              0.0   \n",
       "\n",
       "        pct_no_objective_weeks  \n",
       "0                        100.0  \n",
       "1                        100.0  \n",
       "2                          0.0  \n",
       "3                        100.0  \n",
       "4                          0.0  \n",
       "...                        ...  \n",
       "199995                     0.0  \n",
       "199996                   100.0  \n",
       "199997                   100.0  \n",
       "199998                     0.0  \n",
       "199999                     0.0  \n",
       "\n",
       "[200000 rows x 477 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "base_Edu=pd.read_parquet(\"D:\\\\2AD ENSAE\\\\Projet de statistique appliquée\\\\Dossier travail\\\\base_Edu.parquet\")\n",
    "base_Edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 477 entries, unique_learner_id to pct_no_objective_weeks\n",
      "dtypes: Int64(72), boolean(254), datetime64[us](1), float64(150)\n",
      "memory usage: 450.9 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(base_Edu.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données compte donc 477 variables dont 72 de type Int, 254 de type boolean, 1 de type datetime et 150 de type float soit au total 254 variables qualitatives (**boolean**) et 221 variables quantitatives(**float + int** en excluant les identifiants) sans oublier la variable de type **datetime**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commencons par épurer la base données avant de passer à la réduction de dimension. On s'intéressera aux valeurs manquantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    404.000000\n",
       "mean       0.743390\n",
       "std        0.279055\n",
       "min        0.047720\n",
       "25%        0.666710\n",
       "50%        0.842345\n",
       "75%        0.918625\n",
       "max        1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value=(base_Edu.isnull().sum()/len(base_Edu))\n",
    "missing_value[missing_value>0]\n",
    "missing_value[missing_value>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données comporte donc 404 variables avec valeurs manquantes sur le total des 476 variables de la base ce qui n'est pas du tout négligeable. Voyons combien de valeurs manquantes at-on par type de variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45  variables de type int ont des valeurs manquantes\n",
      "106  variables de type float ont des valeurs manquantes\n",
      "253  variables de type boolean ont des valeurs manquantes\n",
      "0  variables de type datetime ont des valeurs manquantes\n"
     ]
    }
   ],
   "source": [
    "print(base_Edu.select_dtypes(\"int\").isnull().any().sum(),\" variables de type int ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"float\").isnull().any().sum(),\" variables de type float ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"boolean\").isnull().any().sum(),\" variables de type boolean ont des valeurs manquantes\")\n",
    "print(base_Edu.select_dtypes(\"datetime\").isnull().any().sum(),\" variables de type datetime ont des valeurs manquantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seule variable de type datetime n'a donc pas de valeurs manquantes.  Analysons de plus près les proportions de valeurs manquantes des autres types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    253.000000\n",
       "mean       0.903116\n",
       "std        0.072136\n",
       "min        0.695085\n",
       "25%        0.837765\n",
       "50%        0.883305\n",
       "75%        1.000000\n",
       "max        1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_bool=base_Edu.select_dtypes(\"boolean\").isnull().sum()/len(base_Edu)\n",
    "miss_bool[miss_bool>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables de type boolean comporte pratiquement tous assez de valeurs manquantes avec un minimum de 69% et un maximum de 100% du total des observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs manquantes de la base ne sont pas complètement aléatoires. Elles ne sont pas pour la plupart le fruit d'une mauvaise collecte de données. La présence de valeurs manquantes dans cette base est due à diverses raisons notamment le fait que plusieurs individus de la base n'ont pas encore passé d'examen pour le permis. Plusieurs questions(variables) n'ont de sens que dans le cas où le premier examen est passé. Il y a donc une part d'information apporté par ces valeurs manquantes que nous devons inclure dans nos analyses. Pour les variables booléenne, nous pouvons régler ce prblème c'est à dire prendre en compte l'information apportée par ces valeurs manquantes en transformant les variables booléennes en variable catégorielles en considérant les valeurs manquantes comme une catégorie(True=1,False=0 et NA=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(base_Edu.select_dtypes('category'))\n",
    "def encode_column_with_na(col):\n",
    "    mapping = {True: 1, False: 0, pd.NA: 2}  # Encoder les valeurs booléennes et <NA>\n",
    "    return col.map(mapping)\n",
    "colboo=base_Edu.select_dtypes(\"boolean\").columns\n",
    "for col in colboo:\n",
    "    base_Edu[col] = encode_column_with_na(base_Edu[col]).astype(\"category\")\n",
    "#base_Edu[\"is_first_exam_success\"].cat.categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons maintenant aux variables quantitatives (int+float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    151.000000\n",
       "mean       0.475770\n",
       "std        0.292138\n",
       "min        0.047720\n",
       "25%        0.184915\n",
       "50%        0.483995\n",
       "75%        0.736010\n",
       "max        0.941710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_quant=base_Edu.select_dtypes(include=[\"int\",\"float\"]).isnull().sum()/len(base_Edu)\n",
    "miss_quant[miss_quant>0].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les variables quantitatives, la distribution des valeurs manquantes est plus étendue. On enregistre 151 variables qualitatives avec valeurs manquantes. On a un minimum de 4% et un maximum de 94% de valeurs manquantes par rapport au total d'observation(nous rappelons 200000 obervations). La nature des variables et la distribution des valeurs manquantes dans ce cas ne facilite pas la capture de l'information apportée par la présence de ces valeurs manquantes. On pourrait opter pour une imputation par KNN ou par imputation itérative (MICE) mais cela risque de modifier et significativement les corrélations existant déjà entre les variables introduisant ainsi un important biais et au vue de la taille de l'echantillon, cela demanderait un temps d'excution énorme. On choisit ici donc de procéder à une imputation par la médiane après suppression des variables avant un taux élevé de valeurs manquantes pour minimiser le biais. Cela permettra de ne pas trop distordre les distributions existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    118.000000\n",
       "mean       0.054362\n",
       "std        0.074368\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.106310\n",
       "max        0.194530\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miss_quant[miss_quant<=0.20].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choisissons le seuil de 20% de valeurs manquantes. Ce seuil nous parait raisonnable puisqu'il est relativement faible et n'entraine pas la suppression d'un grand nombre de variables quantitatives (118 sont conservées sur un total de 151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppression des variables avec plus de 20% de valeurs manquantes\n",
    "base_Edu=base_Edu.drop(columns=miss_quant[miss_quant>0.20].index)\n",
    "#Imputation par la médiane\n",
    "col=base_Edu.select_dtypes(exclude='category').columns\n",
    "base_Edu[col] = base_Edu[col].fillna(base_Edu[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_Edu.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus aucune valeur manquante. La base de données enfin prête, on peut passer aux opérations de réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_acp=base_Edu.select_dtypes(include=[\"int\",\"float\"]).drop(columns=\"unique_learner_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ploty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "# # Étape 1 : Identifier les colonnes catégorielles\n",
    "# #base_acp=base_Edu.select_dtypes('float').drop(columns=\"unique_learner_id\")\n",
    "# categorical_columns = base_acp.select_dtypes(include=['object', 'category']).columns\n",
    "# numerical_columns = base_acp.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# # Étape 2 : Encodage One-Hot des colonnes catégorielles\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "# encoded_categorical = encoder.fit_transform(base_acp[categorical_columns])\n",
    "# modalities = encoder.get_feature_names_out(categorical_columns)  # Noms des modalités encodées\n",
    "\n",
    "# # Étape 3 : Combiner les colonnes numériques et encodées\n",
    "# import numpy as np\n",
    "# if numerical_columns.any():\n",
    "#     combined_data = np.hstack((base_acp[numerical_columns].values, encoded_categorical))\n",
    "# else:\n",
    "#     combined_data = encoded_categorical\n",
    "\n",
    "# #Centrer et réduire les variables \n",
    "# scaler=StandardScaler()\n",
    "# base_acp=scaler.fit_transform(base_acp)\n",
    "\n",
    "# # Étape 4 : Application de l'ACP\n",
    "# pca = PCA(n_components=10)\n",
    "# components = pca.fit_transform(combined_data)\n",
    " \n",
    "# # Créer un DataFrame avec les trois premiers axes\n",
    "# pca_df = pd.DataFrame(\n",
    "#     components,\n",
    "#     columns=['PCA1', 'PCA2', 'PCA3','PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10']\n",
    "# )\n",
    "\n",
    "# # Étape 5 : Visualisation des composantes principales\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(components[:, 0], components[:, 1], s=100, alpha=0.7, c='blue')\n",
    "# plt.title('Analyse en composantes principale (ACP)')\n",
    "# plt.xlabel('Composante principale 1')\n",
    "# plt.ylabel('Composante principale 2')\n",
    "# plt.grid()\n",
    "# plt.show()\n",
    "\n",
    "# # Étape 6 : Variance expliquée par chaque composante\n",
    "# print(\"Variance expliquée par chaque composante :\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Étape 3 : Récupérer la variance expliquée\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# # Afficher la variance expliquée par axe\n",
    "# for i, var in enumerate(explained_variance, start=1):\n",
    "#     print(f\"Axe {i}: {var * 100:.2f}% de variance expliquée\")\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Graphique de la variance expliquée\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.bar(range(1, len(explained_variance) + 1), explained_variance * 100, color='blue', alpha=0.7)\n",
    "# plt.title('Variance expliquée par axe')\n",
    "# plt.xlabel('Axe')\n",
    "# plt.ylabel('Variance expliquée (%)')\n",
    "# plt.xticks(range(1, len(explained_variance) + 1))\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: prince in d:\\anaconda\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: altair<6.0.0,>=5.0.0 in d:\\anaconda\\lib\\site-packages (from prince) (5.0.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.0 in d:\\anaconda\\lib\\site-packages (from prince) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=1.5.1 in d:\\anaconda\\lib\\site-packages (from prince) (1.5.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from altair<6.0.0,>=5.0.0->prince) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\anaconda\\lib\\site-packages (from altair<6.0.0,>=5.0.0->prince) (4.19.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from altair<6.0.0,>=5.0.0->prince) (1.26.4)\n",
      "Requirement already satisfied: toolz in d:\\anaconda\\lib\\site-packages (from altair<6.0.0,>=5.0.0->prince) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in d:\\anaconda\\lib\\site-packages (from altair<6.0.0,>=5.0.0->prince) (4.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas<3.0.0,>=2.2.0->prince) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas<3.0.0,>=2.2.0->prince) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas<3.0.0,>=2.2.0->prince) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.1->prince) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.1->prince) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn<2.0.0,>=1.5.1->prince) (3.5.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6.0.0,>=5.0.0->prince) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6.0.0,>=5.0.0->prince) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6.0.0,>=5.0.0->prince) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6.0.0,>=5.0.0->prince) (0.10.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.0->prince) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->altair<6.0.0,>=5.0.0->prince) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 373 entries, unique_learner_id to pct_no_objective_weeks\n",
      "dtypes: Int64(46), category(254), datetime64[us](1), float64(72)\n",
      "memory usage: 238.8 MB\n"
     ]
    }
   ],
   "source": [
    "base_Edu.info()\n",
    "base_Edu_sample = base_Edu.sample(n=5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation des colonnes datetime\n",
      "je suis là\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n",
      "d:\\Anaconda\\lib\\site-packages\\prince\\famd.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  eta2[col] = (\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15576\\736214425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mfamd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFAMD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m378\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Conserver 10 dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"je suis là\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbase_Edu_reduced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfamd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_Edu_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'jai fini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Résultat : DataFrame réduit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\prince\\utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;34mf\"The X argument must be a pandas DataFrame, but got {type(X).__name__}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             )\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\prince\\pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, as_array)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \"\"\"\n\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m         \u001b[0mrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mas_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\prince\\utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[1;34mf\"The X argument must be a pandas DataFrame, but got {type(X).__name__}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             )\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\prince\\famd.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             eta2[col] = (\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mni\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             ).values\n\u001b[0;32m     89\u001b[0m         self.column_coordinates_ = pd.concat(\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10372\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10373\u001b[0m         )\n\u001b[1;32m> 10374\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"apply\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  10375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10376\u001b[0m     def map(\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    914\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine_kwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1064\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_numba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1082\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\prince\\famd.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mni\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             eta2[col] = (\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mni\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             ).values\n\u001b[0;32m     89\u001b[0m         self.column_coordinates_ = pd.concat(\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11668\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11669\u001b[0m     ):\n\u001b[1;32m> 11670\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11671\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  12504\u001b[0m         \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12505\u001b[0m     ):\n\u001b[1;32m> 12506\u001b[1;33m         return self._min_count_stat_function(\n\u001b[0m\u001b[0;32m  12507\u001b[0m             \u001b[1;34m\"sum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnansum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12508\u001b[0m         )\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_min_count_stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  12487\u001b[0m             \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 12489\u001b[1;33m         return self._reduce(\n\u001b[0m\u001b[0;32m  12490\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  12491\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  11560\u001b[0m         \u001b[1;31m# After possibly _get_data and transposing, we are now in the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11561\u001b[0m         \u001b[1;31m#  simple case where we can use BlockManager.reduce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11562\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblk_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11563\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor_from_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11564\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"boolean\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1498\u001b[0m         \u001b[0mres_blocks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1500\u001b[1;33m             \u001b[0mnbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1501\u001b[0m             \u001b[0mres_blocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mblk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  11468\u001b[0m                     \u001b[0mdtype_has_keepdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhas_keepdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11469\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mhas_keepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 11470\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m  11471\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  11472\u001b[0m                     warnings.warn(\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py\u001b[0m in \u001b[0;36m_reduce\u001b[1;34m(self, name, skipna, keepdims, **kwargs)\u001b[0m\n\u001b[0;32m   1415\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1417\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(self, axis, min_count, skipna, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m         \u001b[0mvalid_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_valid_sp_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m         \u001b[0msp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_vals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m         \u001b[0mhas_na\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mngaps\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_null_fill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_minimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[0m\u001b[0;32m     48\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# from prince import FAMD\n",
    "\n",
    "\n",
    "# # Identifier les types de colonnes\n",
    "# numeric_cols = base_Edu_sample.select_dtypes(include=['int64', 'float64']).columns\n",
    "# categorical_cols = base_Edu_sample.select_dtypes(include=['category', 'object', 'bool']).columns\n",
    "\n",
    "# # Note : Les colonnes datetime64 ne sont pas directement utilisables dans la FAMD.\n",
    "# # On pourrait les transformer si elles ont du sens (par exemple en extraire l'année, le mois, etc.)\n",
    "# datetime_cols = base_Edu_sample.select_dtypes(include=['datetime64']).columns\n",
    "# #une tab\n",
    "# if len(datetime_cols) > 0:\n",
    "#     print(\"Transformation des colonnes datetime\")\n",
    "#     for col in datetime_cols:\n",
    "#         base_Edu_sample[col + '_year'] = base_Edu_sample[col].dt.year\n",
    "#         base_Edu_sample[col + '_month'] = base_Edu_sample[col].dt.month\n",
    "#         base_Edu_sample[col + '_day'] = base_Edu_sample[col].dt.day\n",
    "#     base_Edu_sample = base_Edu_sample.drop(columns=datetime_cols)\n",
    "\n",
    "# # Application de la FAMD\n",
    "# famd = FAMD(n_components=378, random_state=42)  # Conserver 10 dimensions\n",
    "# print(\"je suis là\")\n",
    "# base_Edu_reduced = famd.fit_transform(base_Edu_sample)\n",
    "# print('jai fini')\n",
    "# # Résultat : DataFrame réduit\n",
    "# base_Edu_reduced = pd.DataFrame(base_Edu_reduced, columns=[f\"FAMD_{i+1}\" for i in range(base_Edu_reduced.shape[1])])\n",
    "\n",
    "# # Afficher les premières lignes\n",
    "# print(base_Edu_reduced.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous commencons d'abord par exclure les identifiants ainsi que la variable de type datetime de la base que nous utiliserons pour notre Auto-Enoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_AE=base_Edu.select_dtypes(exclude=\"datetime\").drop(columns=\"unique_learner_id\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encoding_dim          loss\n",
      "0             3  3.788562e+05\n",
      "1             5  6.318274e+05\n",
      "2             7  1.072575e+06\n",
      "3             9  1.395264e+06\n",
      "4            11  1.106861e+06\n",
      "5            13  1.882605e+06\n",
      "6            15  2.011396e+06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Préparation de la base de données\n",
    "# (Note : 'base_AE' devrait être défini ou chargé préalablement dans ton code)\n",
    "numerical_cols = base_AE.select_dtypes(include=['int', 'float']).columns\n",
    "categorical_cols = base_AE.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Encodage des variables catégoriques avec OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_cats = encoder.fit_transform(base_AE[categorical_cols])\n",
    "\n",
    "# Centrage et réduction des variables numériques\n",
    "scaler = StandardScaler()\n",
    "scaled_nums = scaler.fit_transform(base_AE[numerical_cols])\n",
    "\n",
    "# Fusionner les données encodées et normalisées\n",
    "final_data = np.hstack([encoded_cats, scaled_nums])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "x_train, x_test = train_test_split(final_data, test_size=0.3, random_state=42)\n",
    "\n",
    "# Dimensions de l'entrée et du codage\n",
    "input_dim = x_train.shape[1]  # Nombre de caractéristiques\n",
    "\n",
    "encoding_dimension = [3, 5, 7, 9, 11,13,15]\n",
    "# Liste pour suivre les résultats (dimension latente et perte associée)\n",
    "results = []\n",
    "\n",
    "# Essai de différentes dimensions latentes\n",
    "for encoding_dim in encoding_dimension:  # Tester des dimensions latentes de 1 à 20\n",
    "    # Construction de l'auto-encodeur\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encodeur\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    \n",
    "    # Décodeur\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    \n",
    "    # Créer le modèle\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    \n",
    "    # Compiler le modèle\n",
    "    autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    \n",
    "    # Entraîner le modèle\n",
    "    history = autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test), verbose=0)\n",
    "    \n",
    "    # Récupérer la perte de validation à la dernière époque\n",
    "    final_loss = history.history['val_loss'][-1]\n",
    "    \n",
    "    # Ajouter la dimension et la perte au tableau des résultats\n",
    "    results.append({'encoding_dim': encoding_dim, 'loss': final_loss})\n",
    "\n",
    "# Convertir les résultats en DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n",
    "\n",
    "# encoding_dim = 10  # Dimension de l'espace latent (tu peux ajuster ce nombre)\n",
    "\n",
    "# # Construction de l'Auto-Encoder\n",
    "# # 1. Définir l'entrée du modèle\n",
    "# input_img = Input(shape=(input_dim,))\n",
    "\n",
    "# # 2. Encoder\n",
    "# encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# # 3. Décoder\n",
    "# decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# # 4. Créer l'auto-encodeur\n",
    "# autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# # Compiler le modèle\n",
    "# autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# # Entraînement du modèle\n",
    "# autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "# # 5. Extraire l'encodeur uniquement\n",
    "# encoder = Model(input_img, encoded)\n",
    "\n",
    "# # Visualisation des résultats\n",
    "# encoded_imgs = encoder.predict(x_test)  # Représentation encodée\n",
    "# decoded_imgs = autoencoder.predict(x_test)  # Données reconstruites\n",
    "\n",
    "# # Visualisation des images originales et reconstruites\n",
    "# n = 2  # Nombre d'images à afficher\n",
    "# plt.figure(figsize=(20, 4))\n",
    "\n",
    "# for i in range(n):\n",
    "#     # Données originales\n",
    "#     ax = plt.subplot(2, n, i + 1)\n",
    "#     plt.bar(range(len(x_test[i])), x_test[i])  # Affichage sous forme de barres\n",
    "#     plt.title(f\"Original {i+1}\")\n",
    "#     plt.axis('off')  # Cacher les axes\n",
    "\n",
    "#     # Reconstruction\n",
    "#     ax = plt.subplot(2, n, i + 1 + n)\n",
    "#     plt.bar(range(len(decoded_imgs[i])), decoded_imgs[i])  # Affichage sous forme de barres\n",
    "#     plt.title(f\"Reconstructed {i+1}\")\n",
    "#     plt.axis('off')  # Cacher les axes\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous chosissons ici comme dimension latente k=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "547/547 [==============================] - 6s 9ms/step - loss: 9621.1836 - val_loss: 18852.5547\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 27914.2188 - val_loss: 36517.3984\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 45331.8477 - val_loss: 53175.6836\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 62049.9219 - val_loss: 69295.7422\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 78782.0391 - val_loss: 85288.7969\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 95318.4688 - val_loss: 101463.6094\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 112627.8047 - val_loss: 118076.6797\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 129723.2344 - val_loss: 134901.8906\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 147526.5156 - val_loss: 152340.3750\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 166077.8438 - val_loss: 169663.6562\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 183705.7656 - val_loss: 187688.6094\n",
      "Epoch 12/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 203371.0156 - val_loss: 205309.5625\n",
      "Epoch 13/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 222191.9688 - val_loss: 223225.6406\n",
      "Epoch 14/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 241182.4375 - val_loss: 241821.5469\n",
      "Epoch 15/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 261581.7656 - val_loss: 260444.3281\n",
      "Epoch 16/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 281084.6875 - val_loss: 279190.0938\n",
      "Epoch 17/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 301153.2188 - val_loss: 298336.3125\n",
      "Epoch 18/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 321701.0000 - val_loss: 317963.8750\n",
      "Epoch 19/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 343652.4688 - val_loss: 336854.9375\n",
      "Epoch 20/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 364071.5625 - val_loss: 356989.5000\n",
      "Epoch 21/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 386349.1875 - val_loss: 377399.4062\n",
      "Epoch 22/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 406727.0625 - val_loss: 398270.8750\n",
      "Epoch 23/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 429960.2812 - val_loss: 419379.3750\n",
      "Epoch 24/50\n",
      "547/547 [==============================] - 5s 9ms/step - loss: 453128.5000 - val_loss: 440443.6250\n",
      "Epoch 25/50\n",
      "547/547 [==============================] - 5s 10ms/step - loss: 476529.7188 - val_loss: 462126.6562\n",
      "Epoch 26/50\n",
      "547/547 [==============================] - 5s 8ms/step - loss: 501155.9688 - val_loss: 483526.2188\n",
      "Epoch 27/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 523260.5938 - val_loss: 505685.0312\n",
      "Epoch 28/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 548437.5625 - val_loss: 528582.1875\n",
      "Epoch 29/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 573757.9375 - val_loss: 551434.1875\n",
      "Epoch 30/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 598563.0000 - val_loss: 574969.7500\n",
      "Epoch 31/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 625301.2500 - val_loss: 598932.6875\n",
      "Epoch 32/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 652103.2500 - val_loss: 622837.9375\n",
      "Epoch 33/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 678049.8750 - val_loss: 647452.7500\n",
      "Epoch 34/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 704798.4375 - val_loss: 672838.3750\n",
      "Epoch 35/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 731656.6250 - val_loss: 698091.3125\n",
      "Epoch 36/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 762734.3750 - val_loss: 723285.2500\n",
      "Epoch 37/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 790598.8750 - val_loss: 749893.5625\n",
      "Epoch 38/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 818937.5625 - val_loss: 776437.5625\n",
      "Epoch 39/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 850523.3125 - val_loss: 803590.6250\n",
      "Epoch 40/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 876199.6875 - val_loss: 831104.5000\n",
      "Epoch 41/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 908910.6875 - val_loss: 859170.3125\n",
      "Epoch 42/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 939737.3750 - val_loss: 886556.5625\n",
      "Epoch 43/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 970175.8125 - val_loss: 915023.6875\n",
      "Epoch 44/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 1002824.2500 - val_loss: 942282.7500\n",
      "Epoch 45/50\n",
      "547/547 [==============================] - 5s 8ms/step - loss: 1032429.1875 - val_loss: 971340.9375\n",
      "Epoch 46/50\n",
      "547/547 [==============================] - 4s 8ms/step - loss: 1063686.7500 - val_loss: 1000049.0000\n",
      "Epoch 47/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 1096382.3750 - val_loss: 1029412.8750\n",
      "Epoch 48/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 1130596.0000 - val_loss: 1058878.8750\n",
      "Epoch 49/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 1163819.2500 - val_loss: 1088284.7500\n",
      "Epoch 50/50\n",
      "547/547 [==============================] - 4s 7ms/step - loss: 1195609.2500 - val_loss: 1119569.5000\n",
      "1875/1875 [==============================] - 4s 2ms/step\n",
      "1875/1875 [==============================] - 5s 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur moyenne de reconstruction : 0.6658\n"
     ]
    }
   ],
   "source": [
    "# Fixer la dimension latente optimale\n",
    "encoding_dim = 7\n",
    "\n",
    "# Construction de l'auto-encodeur\n",
    "input_img = Input(shape=(input_dim,))\n",
    "\n",
    "# Encodeur\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# Décodeur\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Création du modèle auto-encodeur\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# Compiler le modèle\n",
    "autoencoder.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = autoencoder.fit(x_train, x_train, \n",
    "                          epochs=50, \n",
    "                          batch_size=256, \n",
    "                          shuffle=True, \n",
    "                          validation_data=(x_test, x_test), \n",
    "                          verbose=1)\n",
    "\n",
    "# Extraire uniquement l'encodeur pour obtenir la représentation dans l'espace latent\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# Obtenir les représentations encodées (dans l'espace latent)\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "\n",
    "# Reconstruire les images (après passage par l'auto-encodeur)\n",
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "# Visualisation des performances\n",
    "n = 5  # Nombre de cas à visualiser\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "for i in range(n):\n",
    "    # Données originales\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.bar(range(len(x_test[i])), x_test[i])\n",
    "    plt.title(f\"Original {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Reconstructions\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.bar(range(len(decoded_imgs[i])), decoded_imgs[i])\n",
    "    plt.title(f\"Reconstructed {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Analyse des erreurs de reconstruction\n",
    "reconstruction_error = np.mean((x_test - decoded_imgs) ** 2, axis=1)\n",
    "print(f\"Erreur moyenne de reconstruction : {np.mean(reconstruction_error):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 [==============================] - 14s 2ms/step\n",
      "Forme des nouvelles variables : (200000, 7)\n"
     ]
    }
   ],
   "source": [
    "# Extraire l'encodeur\n",
    "encoder = Model(autoencoder.input, autoencoder.layers[-2].output)\n",
    "\n",
    "# Transformer les données d'entrée en espace latent\n",
    "latent_variables = encoder.predict(final_data)\n",
    "\n",
    "# Vérifier les dimensions des nouvelles variables\n",
    "print(\"Forme des nouvelles variables :\", latent_variables.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latent_Var_1  Latent_Var_2  Latent_Var_3  Latent_Var_4  Latent_Var_5  \\\n",
      "0   7910.012695   7960.086914   8005.469727   7907.571777   7742.548340   \n",
      "1   7911.761230   7960.165527   8008.788574   7903.861328   7743.288574   \n",
      "2   7795.596680   7731.472656   7679.855469   7763.818359   7929.888672   \n",
      "3   7807.552734   7834.776367   7857.964355   7811.741699   7747.300293   \n",
      "4   7750.297852   7728.935547   7711.377441   7773.888184   7879.707031   \n",
      "\n",
      "   Latent_Var_6  Latent_Var_7  \n",
      "0   7761.066895   7766.777832  \n",
      "1   7760.366211   7764.662598  \n",
      "2   7917.424316   7907.690918  \n",
      "3   7751.700195   7752.699219  \n",
      "4   7863.044434   7855.377930  \n"
     ]
    }
   ],
   "source": [
    "# Créer un DataFrame à partir des nouvelles variables\n",
    "latent_df = pd.DataFrame(latent_variables, columns=[f\"Latent_Var_{i+1}\" for i in range(latent_variables.shape[1])])\n",
    "\n",
    "# Afficher un aperçu\n",
    "print(latent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contribution de la Latent_Var_1: 11.82%\n",
      "Contribution de la Latent_Var_2: 22.15%\n",
      "Contribution de la Latent_Var_3: 41.45%\n",
      "Contribution de la Latent_Var_4: 6.13%\n",
      "Contribution de la Latent_Var_5: 7.81%\n",
      "Contribution de la Latent_Var_6: 5.80%\n",
      "Contribution de la Latent_Var_7: 4.84%\n"
     ]
    }
   ],
   "source": [
    "# Variance de chaque variable latente\n",
    "latent_variances = np.var(latent_variables, axis=0)\n",
    "\n",
    "# Contribution proportionnelle de chaque variable latente\n",
    "latent_contributions = latent_variances / np.sum(latent_variances)\n",
    "\n",
    "# Affichage des contributions\n",
    "for i, contrib in enumerate(latent_contributions):\n",
    "    print(f\"Contribution de la Latent_Var_{i+1}: {contrib:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
